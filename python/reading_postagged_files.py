# -*- coding: utf-8 -*-
"""reading_postagged_files.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/13GAGqgvHTwM3vjvsmepySt4TGqVCTyfT
"""

# importing libraries
import re
import pandas as pd
import glob
from datetime import datetime

# setting up the paths
path = "C:\\Users\\joaop\\Desktop\\Corpora\\C-Oral-Brasil\\PoSTagged\\SimplifiedPoStagged_files_txt\\"
posTranscriptionList = glob.glob(f"{path}\\*.txt")
textOutputPath = "C:\\Users\\joaop\\Desktop\\Corpora\\C-Oral-Brasil\\dissertation_codes\\text_output_files\\"

# this function reads, preprocess and convert (into a dataframe) the texts of the simple transcriptions

def read_preprocessFile(TranscriptionList): #str
    dfList = []
    newTurnList = []
    for file in TranscriptionList:
        fileOpen = open(file, "r", encoding="utf-8") # , encoding="utf-8-sig"
        fileName = file.split("\\")[8][:8]
        text = fileOpen.readlines()
        # listTurnList.append(text)
        # fileNameList.append(fileName)
        for turn in text:
            turnClean = re.sub(" +", " ", turn) # replace extra spaces for one
            turnClean = re.sub("\+", "_utt", turnClean) #replace interruption marks by "_utt"
            turnClean = re.sub("\/\/", "_utt", turnClean) #replace terminal break marks by "_utt"
            turnClean = re.sub("\n", "_utt", turnClean) #replace line breaks by "_utt"
            # turnClean = re.sub(" $", "_utt", turnClean)
            turnClean = re.sub("_utt _utt", "_utt", turnClean)#replace double "_utt" by single "_utt"
            turnClean = re.sub("_utt_utt", "_utt", turnClean) #replace double "_utt" by single "_utt"
            # turnClean = re.sub(" $", "", turnClean)
            turnClean = re.sub("\*[A-Z]{3}:", "", turnClean)#delete speaker's code
            turnClean = re.sub("\[[0-9]+\]", "", turnClean)#delete utterance number
            turnClean = re.sub(r"o\\\'", "o'", turnClean) #replace "o\'" by  "o'"
            turnClean = re.sub(r"\*retract:meio_de_ \$\>", "", turnClean) #replace *retract:meio_de_ $\> by nothing
            turnClean = re.sub("([a-zA-ZÀ-ÿũ]+)(\[[a-zA-ZÀ-ÿũ]+\])", r"\1 \2", turnClean) # fix errors such as 'três[três]'
            turnClean = re.sub("(\*|)[A-Z]{3}(:||\s+)\/\/", "", turnClean) # fix erros such as FLA: //
            turnClean = re.sub("(\*|)[A-Z]{3}(:||\s+)\n", "", turnClean) # fix erros such as *FLA: \n
            turnClean = re.sub("\&he", "", turnClean) # replace &he (hesitation) by nothing
            turnClean = re.sub("\&[a-zA-ZÀ-ÿũ]+", "", turnClean) # replace interrupted words by nothing
            turnClean = re.sub(" celerando\s+\[[a-zA-ZÀ-ÿũ]+\]", " celerando OALT acelerando [acelerar]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("cordava\s+\[corda\]", "cordava OALT acordava [acordar]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("creditei\s+\[creditar\]", "creditei OALT acreditei [acreditar]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("doro\s+OALT\s+dolo\s+\[dolo\]", "doro OALT adoro [adorar]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("lisou \[Lisa\] \<V PS 3S IND VFIN\> \[liso\] \<V PS 3S IND VFIN\>", "lisou OALT alisou [alisar] <V PS 3S IND VFIN>", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("trapalhou \[trapa\]", "trapalhou OALT atrapalhou [atrapalhar]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("borrecido \[borra\]", "borrecido OALT aborrecido [aborrecer]", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("teirinho \[teiró\] \<N M S\>", "teirinho OALT inteirinho [inteiro] <ADJ M S>", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("trevidão \[trevo\] \<N F S\>", "trevidão OALT atrevidão [atrever] <ADJ M S>", turnClean) # fix errors of annotation of apheretic forms
            turnClean = re.sub("\>\>", "\>", turnClean) # replace doblue > by one >
            turnClean = re.sub("\<\<", "\<", turnClean) # replace doblue < by one >

            abb = turn.split(":")[0][1:] #getting the speaker's coded
            # abbreviationList.append(fileName+abb)
            newTurnList.append([abb, fileName, turnClean]) #appending a list as an element of a list
    textDf = pd.DataFrame(newTurnList, columns=["acronym", "file", "turn"]) #creating dataframe

    dfList.append(textDf) #appeding dataframes into a list
    allSimpleUttDf = pd.concat(dfList) #merging all Dfs
    return allSimpleUttDf

allPosUtt = read_preprocessFile(posTranscriptionList) #applying the function

allPosUtt["utterance"] = allPosUtt.turn.str.split("_utt") #splitting the turns as a list of utterances
allPosUtt = allPosUtt.explode("utterance") # one row = one utterance
allPosUtt.utterance = allPosUtt.utterance.str.replace("_utt", "") # making sure that the "_utt" marker is out of the text
allPosUtt = allPosUtt[allPosUtt["utterance"]!=""] # deleting unwanted/unnecessary rows
allPosUtt = allPosUtt[allPosUtt["utterance"]!=" "] # deleting unwanted/unnecessary rows
allPosUtt = allPosUtt[allPosUtt["utterance"]!="\n"] # deleting unwanted/unnecessary rows

allPosUtt["person_code"] = allPosUtt.file+allPosUtt.acronym # creating a code per person by file
allPosUtt["utterance"] = allPosUtt["utterance"].str.replace(" \/ ", " ") # deleting the non-terminal break markup
allPosUtt["utterance"] = allPosUtt["utterance"].str.replace("ü", "u") #replacing U with diacritics by 'normal' u
allPosUtt["utterance"] = allPosUtt["utterance"].str.replace("Ü", "U") #replacing U with diacritics by 'normal' u

print(f"The size of the current dataframe is {len(allPosUtt)}\nThe number of utterances in C-oral is 34167\nThere are {34167- len(allPosUtt)} utterances missing")
allPosUtt.tail()

def combineAOLTFormsSimple(string):
    # it transforms "nũ OALT não" (and alike) to "nũ-OALT-não" so that it can be counted as one word
    # it does the same thing to lemmas and pos tags
    words = string.split()
    for i in range(len(words)):
        if "OALT" == words[i]: # and not "[a-zA-ZÀ-ÿũ]+-[0-9]" in words[i-1]
            words[i] = words[i-1] + "-OALT-" + words[i+1]
            words[i-1] = ""
            words[i+1] = ""
    newString = re.sub('\s+',' ', " ".join(words))
    #                         em              [em]             <PRP> num-2-OALT-um [um]      <DET M S>
    newString = re.sub("([a-zA-ZÀ-ÿũ]+)\s+(\[[a-zA-ZÀ-ÿũ]+\])\s+(\<[a-zA-ZÀ-ÿũ]+\>)\s+([a-zA-ZÀ-ÿũ]+)-[0-9]-(OALT-[a-zA-ZÀ-ÿũ]+)\s+(\[[a-zA-ZÀ-ÿũ]+\])\s+(\<.*?\>)", r"\1-\4-\5 \2-\6 \3-\7", newString)
    newString = re.sub("([a-zA-ZÀ-ÿũ]+)-(OALT)-([a-zA-ZÀ-ÿũ]+)-[a-zA-ZÀ-ÿũ]+-OALT-([a-zA-ZÀ-ÿũ]+)", r"\1-\2-\3-\4", newString)
    newString = re.sub("\[([a-zA-ZÀ-ÿũ]+)\]-\[([a-zA-ZÀ-ÿũ]+)\]", r"[\1-\2]", newString)
    newString = re.sub("\<([a-zA-ZÀ-ÿũ]+)\>-\<(.*?)\>", r"<\1-\2>", newString)
    return newString

allPosUtt["utterance_with_OALT"] = allPosUtt["utterance"].apply(lambda x: combineAOLTFormsSimple(x)) #applying the function

# f = allPosUtt[allPosUtt["utterance_with_OALT"].str.contains("inho ")]
# repr(f.iloc[70,5])

# creating a column with only the textual information from the utterance_with_OALT column
allPosUtt["clean_utterance"] = allPosUtt["utterance_with_OALT"].str.replace("\<.*?\>", " ") #deleting tags
allPosUtt["clean_utterance"] = allPosUtt["clean_utterance"].str.replace("\[[a-zA-ZÀ-ÿũ]+(\=||-)[a-zA-ZÀ-ÿũ]+\]|\[[a-zA-ZÀ-ÿũ]+\]|\[[a-zA-ZÀ-ÿũ]+=[a-zA-ZÀ-ÿũ]+=[a-zA-ZÀ-ÿũ]+\]|\[\w+-\w+(?:-\w+|-\w+-\w+|-\w+-\w+-\w+|-\w+-\w+-\w+-\w+|)\]", " ") #deleting lemmas
allPosUtt["clean_utterance"] = allPosUtt["clean_utterance"].str.replace(" \/ ", " ") #deleting non-terminal break symbols
allPosUtt["clean_utterance"] = allPosUtt["clean_utterance"].str.replace("\s+", " ") #deleting extra spaces

# creating a column only for the lemmas from the utterance_with_OALT column
allPosUtt["lemmas"] = allPosUtt.utterance_with_OALT.apply(lambda x: [x.group() for x in re.finditer("\[[a-zA-ZÀ-ÿũ]+(\=||-)[a-zA-ZÀ-ÿũ]+\]|\[[a-zA-ZÀ-ÿũ]+\]|\[[a-zA-ZÀ-ÿũ]+=[a-zA-ZÀ-ÿũ]+=[a-zA-ZÀ-ÿũ]+\]", x)])
allPosUtt["lemmas"] = allPosUtt["lemmas"].apply(lambda y: " ".join(y))
allPosUtt["lemmas"] = allPosUtt["lemmas"].str.replace("[", "")
allPosUtt["lemmas"] = allPosUtt["lemmas"].str.replace("]", "")
allPosUtt["lemmas"] = allPosUtt["lemmas"].str.replace("\s+", " ")

# creating a column only for the POS tagging from the utterance_with_OALT column
allPosUtt["pos_tagged"] = allPosUtt.utterance_with_OALT.apply(lambda x: [x.group() for x in re.finditer("\<.*?\>", x)])
allPosUtt["pos_tagged"] = allPosUtt["pos_tagged"].apply(lambda y: " ".join(y))
allPosUtt["pos_tagged"] = allPosUtt["pos_tagged"].str.replace("\s+", " ")

# creating a column with token and tags from utterance_with_OALT column
# está saindo assim: três_<NUM M P> times_<N M P> por=enquanto [por=enquanto] <ADV> - consertar
allPosUtt["words_with_tags"] = allPosUtt.utterance_with_OALT.str.replace("([a-zA-ZÀ-ÿũ]+-[a-zA-ZÀ-ÿũ]+-[a-zA-ZÀ-ÿũ]+-[a-zA-ZÀ-ÿũ]+)\s+(\[.*?\])\s+(\<.*?\>)", r"\1_\3")
allPosUtt["words_with_tags"] = allPosUtt.words_with_tags.str.replace("([a-zA-ZÀ-ÿũ]+)\s+(\[[a-zA-ZÀ-ÿũ]+\])\s+(\<.*?\>)", r"\1_\3")
allPosUtt["words_with_tags"] = allPosUtt["words_with_tags"].str.replace(" \/ ", " ")

def deleteExtraWhiteSpaces(dataframe):
    #delete extra whitespaces
    for column in dataframe:
        dataframe[column] = dataframe[column].astype(str) # converting the dataframe column types to string
        dataframe[column] = dataframe[column].str.strip() #  and deleting extra white spaces
    return dataframe

allPosUtt_stripped = deleteExtraWhiteSpaces(allPosUtt) #applying the function

# exporting
date = datetime.now().strftime('%Y-%m-%d')
allPosUtt_stripped.to_csv(textOutputPath+f"allPosUtterancesDf_csv_{date}.csv")
allPosUtt_stripped.to_excel(textOutputPath+f"allPosUtterancesDf_excel_{date}.xlsx")