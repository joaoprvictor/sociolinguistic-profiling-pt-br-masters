# -*- coding: utf-8 -*-
"""extracting_info_from_postagged_texts.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5aEmzCdTdCKwyRvG1T1DV9wM0zrLmoJ
"""

#importing libraries
import re


import pandas as pd
# import glob
from datetime import datetime

# assigning the folder that is going to store the output
textOutputPath = "C:\\Users\\joaop\\Desktop\\Corpora\\C-Oral-Brasil\\dissertation_codes\\text_output_files\\"

# reading the pos tagged texts# reading the csv file from the "reading_postagged_files.ipynb"
# In our case, the CSV file was read but if someone chooses to work with the Excel file,
# the function pd.read_csv() should be replaced by pd.read_excel()
date = datetime.now().strftime('%Y-%m-%d')

#in order to get the correct file, the CSV file must have been created in the same day you're using this script
#posTagUtterancesDf_stripped = pd.read_csv(textOutputPath+f"allPosUtterancesDf_csv_{date}.csv")
posTagUtterancesDf_stripped = pd.read_csv(textOutputPath+f"allPosUtterancesDf_csv_2023-05-21.csv")
# changing the dataframe value type (without this, the next step may show an error)
posTagUtterancesDf_stripped = posTagUtterancesDf_stripped.astype(str)

# counting the number of words in each utterance
posTagUtterancesDf_stripped["number_words_per_utterance"] = posTagUtterancesDf_stripped.clean_utterance.apply(lambda x: len(x.split(" ")))

# each row represents an utterance, but if we add a substring to it, it'll be easier to count the number of utterances
#adding the substring
posTagUtterancesDf_stripped["clean_utterance"] = posTagUtterancesDf_stripped["clean_utterance"] + "_EndUtterance"

#counting the number of the substring
posTagUtterancesDf_stripped["utterance_count"] = posTagUtterancesDf_stripped["clean_utterance"].str.count("_EndUtterance")

#deleting the substring from the "clean_utterance" column
posTagUtterancesDf_stripped["clean_utterance"] = posTagUtterancesDf_stripped["clean_utterance"].str.replace("_EndUtterance", "")

"""### verbal conjugation by utterance"""

# negation particle pronunciation and verbal conjugation would be better captured if done by utterance

# VERBAL CONJUGATION
def countVerbsNotStandardForms(string):
    '''
    it counts the number of not standard verb conjugations
    '''
    # list with the words
    conjugationVerbs = ['a\'', r'acabamo\b', r'achamo\b', r'agradecemo\b', r'aprendemo\b', r'arrumamo\b', r'assinávamo\b', r'atravessamo\b', r'\bavi\b',
                    r'\bavinha\b', r'bebemo\b', r'beijamo\b', r'botemo\b', r'chegamo\b', r'cheguemo\b', r'chorano\b', r'colocamo\b', r'começamo\b',
                    r'comemo\b', r'comemoramo', r'compramo\b', r'conhecemo\b', r'conseguimo\b', r'contamo\b', r'conversamo\b', r'corremo\b', r'cortamo\b',
                    r'deixamo\b', r'descansamo\b', r'descemo\b', r'devemo\b', r'empuramo\b', r'encontramo\b', r'entramo\b', r'\benvem\b', r'\benvinha\b',
                    r'escolhemo\b', r'esquecemo\b', r'estamo\b', r'estudemo\b', r'\bevem\b', r'falamo\b', r'\bfazido\b', r'ficamo\b', r'\bfize\b',
                    r'fizemo\b', r'\bfomo\b', r'\bfor\b', r'\bfraga\b', r'\bfragando\b', r'\bfrago\b', r'\bfumo\b', r'ganhamo\b', r'levamo\b',
                    r'levantamo\b', r'levantemo\b', r'mandamo\b', r'mandamo\b', r'\bmanti\b', 'o\'', r'\bparamo\b', r'passamo\b', r'\bpedimo\b',
                    r'peguemo\b', r'perdemo\b', r'\bpinchando\b', r'pintemo\b', r'podemo\b', r'precisamo\b', r'pusemo\b', r'resolvemo\b',
                    r'\bsaímo\b', r'\bseje\b', r'\bsentamo\b',r'\bsentemo\b', r'separamo\b', r'\bsomo\b', r'\bsufro\b', r'\btemo\b',
                    r'\btiramo\b', r'\btemo\b', r'\btiramo\b', r'\btivemo\b', r'\btomamo\b', r'trabalhamo\b', r'\btrago\b', r'\bvesse\b',
                    r'\bviemo\b', r'\bvimo', r'\btó\b', r'\bvamo\b', r'\beu\b tem \bde\b', r'\beu\b tem \bque\b', r'\beixa\b', r'\bxá\b']
                    # "ver" as future of "vir" and "fumo" as fomos weren't found. They aren't in the list

    matches = [] #empty list to store the results
    for form in conjugationVerbs:
        # for each form in the list, check if it's in the string.
        # append the new list into the matches list
        match = re.findall(form, string)
        matches.append(match)


    # the matches list is a list of lists. The line below converts it to a one single list
    matchesFinal = [item for sublist in matches for item in sublist]
    return len(matchesFinal)
    # in case you just want to know the specific verbs, you can put the "return" command directly with the lists without the len()

#applying the function
posTagUtterancesDf_stripped["count_nonstandard_verb_conjugation"] = posTagUtterancesDf_stripped.clean_utterance.apply(lambda x: countVerbsNotStandardForms(x))

"""### verbal agreement by utterance"""

def countVerbAgreementNonStandard(string):
    '''
    it counts the number of not standard verb agreement, for example "os meninos fez"
    '''
    conj = r'(\w+_\<(?:PERS|N|ADJ|PROP|SPEC|DET|NUM)(?:\>|\s+(?:M|F|M\/F)\s+(?:\d+\w+\s+\w+|\w+)\>))(?:.*?|)\s+(\w+_\<V\s(?!GER|INF|PCP).*?\>)' #token_pos tags
    # the above regex is capturing words with tags of personal pronouns, nouns, adjectives, proper names, specifiers, determiners OR numerals that
    # are immediately (or not) followed by a verb that is NOT in one of the nominal forms (gerund, infinitive and participle)
    matches = re.findall(conj, string, re.UNICODE) # find the patterns based on the regex (it returns a list of tuples)
    listConjugation = [list(elem) for elem in matches] #convert the list of tuples as a list of lists
    nonstandard = [] #empty list to store the nonstandard forms
    standard = [] #empty list to store the standard forms
    for lista in listConjugation:
        # the 2nd element of the inner list is the verb conjugation. Splitting by space, we have a list with each verb tag as a list item
        verblist = lista[1].split(" ")
        # the verb number is in the 3rd element of the list
        verbnumber = verblist[2]
        # for each nominal tag put in the regex: if the tag is in the first inner list item and the tag <V is in the second element
        # then, capture the nominal element number.
        # if the verb number is DIFFERENT from the nominal element number, append their number in the nonstandard list
        # if the verb number is EQUAL from the nominal element number, append their number in the standard list
        if re.findall("_<PERS", lista[0]) and "_<V" in lista[1]:
            perslist = lista[0].split(" ")
            persnumber = perslist[2]
            if verbnumber != persnumber:
                nonstandard.append([persnumber, verbnumber])
            else:
                standard.append([persnumber, verbnumber])

        elif re.findall("_<PROP>", lista[0]) and "_<V" in lista[1]:
            propnumber = 'S'
            if verbnumber[1] != propnumber:
                nonstandard.append([verbnumber, propnumber])
            else:
                standard.append([verbnumber, propnumber])

        elif re.findall(r"_<PROP\b", lista[0]) and "_<V" in lista[1]:
            prop2list = lista[0].split(" ")
            prop2number = prop2list[2][0]
            if verbnumber[1] != prop2number:
                nonstandard.append([verbnumber, prop2number])
            else:
                standard.append([verbnumber, prop2number])

        elif re.findall(r"_<N", lista[0]) and "_<V" in lista[1]:
            nounlist = lista[0].split(" ")
            nounnumber = nounlist[2][0]
            if verbnumber[1] != nounnumber:
                nonstandard.append([verbnumber, nounnumber])
            else:
                standard.append([verbnumber, nounnumber])

        elif re.findall(r"_<ADJ", lista[0]) and "_<V" in lista[1]:
            adjlist = lista[0].split(" ")
            adjnumber = adjlist[2][0]
            if verbnumber[1] != adjnumber:
                nonstandard.append([verbnumber, adjnumber])
            else:
                standard.append([verbnumber, adjnumber])

        elif re.findall("_<SPEC", lista[0]) and "_<V" in lista[1]:
            speclist = lista[0].split(" ")
            specnumber = speclist[2][0]
            if verbnumber[1] != specnumber:
                nonstandard.append([verbnumber, specnumber])
            else:
                standard.append([verbnumber, specnumber])

        elif re.findall("_<DET", lista[0]) and "_<V" in lista[1]:
            detlist = lista[0].split(" ")
            detnumber = detlist[2][0]
            if verbnumber[1] != detnumber:
                nonstandard.append([verbnumber, detnumber])
            else:
                standard.append([verbnumber, detnumber])

        elif re.findall("_<NUM", lista[0]) and "_<V" in lista[1]:
            numlist = lista[0].split(" ")
            numnumber = numlist[2][0]
            if verbnumber[1] != numnumber:
                nonstandard.append([verbnumber, numnumber])
            else:
                standard.append([verbnumber, numnumber])

    if len(nonstandard) > 0: #if the size of the nonstandard list is larger than 0, returns its size. If not, returns 0
        return len(nonstandard)
    else:
        return 0

#applying the function
posTagUtterancesDf_stripped["count_nonstandard_verb_agreement"] = posTagUtterancesDf_stripped.words_with_tags.apply(lambda x: countVerbAgreementNonStandard(x))

"""### negation"""

def countNegation(string):
    '''
    it counts the number of negation particles: double negation, nũ, n' era, n' é and não
    '''
    negationsNonStandardList = [r"nũ .*? (?!não|nũ)", "n\' é", "n\' era", r"não .*? não", r"nũ .*? não"]
    # the regex '.*? (?!não|nũ)' restricts to utterances in each nũ is NOT used in double negation
    matches = [] # empty list to store the results
    for form in negationsNonStandardList:
        # for each form in the list, check if it's in the string.
        # append the new list into the matches list
        match = re.findall(form, string)
        matches.append(match) # it makes "matches" as a list of lists

    import itertools
    nonEmptyMatches = [lst for lst in matches if lst] # deleting empty lists
    finalNonStandard = list(itertools.chain(*nonEmptyMatches)) #converting the list of lists into one single list


    # negationsStandardList = [r"(não .*?) (?!não|nũ)"]
    # matches = [] # empty list to store the results
    # for form in negationsStandardList:
    #     # for each form in the list, check if it's in the string.
    #     # append the new list into the matches list
    #     match = re.findall(form, string)
    #     matches.append(match) # it makes "matches" as a list of lists

    # import itertools
    # nonEmptyMatches = [lst for lst in matches if lst] # deleting empty lists
    # finalStandard = list(itertools.chain(*nonEmptyMatches)) #converting the list of lists into one single list

    if len(finalNonStandard) > 0:
        # NegationRatio = len(finalNonStandard)/(len(finalNonStandard)+len(finalStandard))
        # roundRatio = round(NegationRatio, 2)
        return len(finalNonStandard)
    else:
        return 0

posTagUtterancesDf_stripped["negation"] = posTagUtterancesDf_stripped.clean_utterance.apply(lambda x: countNegation(x))

"""### plural marking in noun phrases"""

def countNonStandardPlural(string):
    '''
    it counts the numbers of non-standard plural agreement in nominal groups with DET, NUM and N
    '''
    nominalGroup = r'(?:\w+_<DET\s+(?:M|F|M\/F)\s+(P|S)>\s+|)(?:\w+_<NUM\s+(?:M|F|M\/F)\s+(S|P)>\s+|)\w+_<N\s+(?:M|F|M\/F)\s+(P|S)>' #token_pos tags
    matches = re.findall(nominalGroup, string, re.UNICODE) # look for matches with the regex
    listPlurals = [list(elem) for elem in matches] #convert the list of tuples as a list of lists
    newListPlurals = [[item for item in lst if item != ''] for lst in listPlurals] # removes any empty items from the list of lists

    bools = [] # empty list to store the bool values
    for lista in newListPlurals:
        # for each minilist, check if the elements are the same (if they are the same, it means the agreement is standard)
        # append the results (true or false) in the bools list
        same_items = all(x == lista[0] for x in lista)
        bools.append(str(same_items))

    NonStandardPlural = [] #false list
    StandardPlural = [] #true list
    for item in bools:
        # for each item in the list if the item is "true", append it to the Standard list, if "false" is, put it in the NonStandard list
        if "True" in item:
            StandardPlural.append("StandardPlural")
        if "False" in bools:
            NonStandardPlural.append("NonStandardPlural")

    return len(NonStandardPlural)
# in case you just want to know the specific number combinations, you can put the "return" command directly with the list without the len()

posTagUtterancesDf_stripped["non_standard_plural"] = posTagUtterancesDf_stripped.words_with_tags.apply(lambda x: countNonStandardPlural(x))

"""### Continuing preprocessing and extracting info"""

# grouping all the selected columns (that are between the [[]]) according to the "person_code" column
# resetting the index that will be related to new dataframe
posTagUtteranceWithInfoDf = posTagUtterancesDf_stripped.groupby('person_code')[['number_words_per_utterance', 'acronym', 'utterance',
       'lemmas', 'pos_tagged', 'clean_utterance', 'file', 'utterance_count', "words_with_tags", 'count_nonstandard_verb_conjugation',
       "count_nonstandard_verb_agreement", 'negation', 'non_standard_plural']].agg(lambda x: ' '.join(map(str, x))).reset_index()

# counting the number of words per speaker
posTagUtteranceWithInfoDf["total_words"] = posTagUtteranceWithInfoDf['number_words_per_utterance'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_words"] = posTagUtteranceWithInfoDf["total_words"].apply(lambda x: sum(int(i) for i in x))

# ========
# counting the number of utterances per speaker
posTagUtteranceWithInfoDf["total_utterance"] = posTagUtteranceWithInfoDf['utterance_count'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_utterance"] = posTagUtteranceWithInfoDf["total_utterance"].apply(lambda x: sum(int(i) for i in x))

# counting the number of nonstandard verb conjugation per speaker
posTagUtteranceWithInfoDf["total_nonstandard_verb_conjugation"] = posTagUtteranceWithInfoDf['count_nonstandard_verb_conjugation'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_nonstandard_verb_conjugation"] = posTagUtteranceWithInfoDf["total_nonstandard_verb_conjugation"].apply(lambda x: sum(int(i) for i in x))

# counting the number of nonstandard verb agreement per speaker
posTagUtteranceWithInfoDf["total_nonstandard_verb_agreement"] = posTagUtteranceWithInfoDf['count_nonstandard_verb_agreement'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_nonstandard_verb_agreement"] = posTagUtteranceWithInfoDf["total_nonstandard_verb_agreement"].apply(lambda x: sum(int(i) for i in x))

# counting the number of negation ratio per speaker
posTagUtteranceWithInfoDf["total_count_negation"] = posTagUtteranceWithInfoDf['negation'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_count_negation"] = posTagUtteranceWithInfoDf["total_count_negation"].apply(lambda x: sum(float(i) for i in x))

# counting the number of negation ratio per speaker
posTagUtteranceWithInfoDf["total_non_standard_plural"] = posTagUtteranceWithInfoDf['non_standard_plural'].apply(lambda x: x.split(" "))
posTagUtteranceWithInfoDf["total_non_standard_plural"] = posTagUtteranceWithInfoDf["total_non_standard_plural"].apply(lambda x: sum(float(i) for i in x))

#deleting extra repetitions from the columns "acronym" and "file"
posTagUtteranceWithInfoDf["acronym"] = posTagUtteranceWithInfoDf["acronym"].apply(lambda x: x.split(" ")[0])
posTagUtteranceWithInfoDf["file"] = posTagUtteranceWithInfoDf["file"].apply(lambda x: x.split(" ")[0])

# selecting and reordering the columns
posTagUtteranceWithInfoDf = posTagUtteranceWithInfoDf[["file", "acronym", "person_code", "utterance", "clean_utterance", "words_with_tags", "lemmas", "pos_tagged", "total_words", "total_utterance", "total_nonstandard_verb_agreement","total_nonstandard_verb_conjugation", "total_count_negation", "total_non_standard_plural"]]

"""#### apheresis"""

def listingApheresis(string):
    '''
    it returns a list with all apheretic forms found in a string
    '''
    # this function lists all of the apheretic forms in the string
    apheresisVerbs = ['babacar-OALT-embabacar', 'baixa-OALT-abaixa','baixar-OALT-abaixar', 'baixei-OALT-abaixei','caba-OALT-acaba', 'cabar-OALT-acabar',
                  'cabava-OALT-acabava', 'cabei-OALT-acabei', 'cabou-OALT-acabou', 'celera-OALT-acelera', 'celerando-OALT-acelerando',
                    'certar-OALT-acertar', 'chei-OALT-achei', 'cho-OALT-acho', 'contece-OALT-acontece', 'contecer-OALT-acontece', 'conteceu-OALT-aconteceu',
                    'cordava-OALT-acordava', 'creditei-OALT-acreditei', 'dianta-OALT-adianta', 'doro-OALT-adoro',
                   'fundar-OALT-afundar', 'garrou-OALT-agarrou', 'guenta-OALT-aguenta', 'guentando-OALT-aguentando', 'guentar-OALT-aguentar',
                   'guento-OALT-aguento', 'guentou-OALT-aguentou','judar-OALT-ajudar','lisou-OALT-alisou', 'magina-OALT-imagina', 'mamentar-OALT-amamentar',
                   'marrava-OALT-amarrava', 'panhava-OALT-apanhava', 'parece-OALT-aparece', 'pareceu-OALT-apareceu', 'pera-OALT-espera',
                   'perta-OALT-aperta', 'pertar-OALT-apertar', 'pertei-OALT-apertei', 'proveita-OALT-aproveita', 'proveitando-OALT-aproveitando',
                   'purra-OALT-empurra', 'rancaram-OALT-arrancaram', 'rancava-OALT-arrancava', 'rancou-OALT-arrancou', 'ranjar-OALT-arranjar',
                   'ranjasse-OALT-arranjasse', 'ranjou-OALT-arranjou', 'rebentando-OALT-arrebentando', 'rebentar-OALT-arrebentar',
                   'rumaram-OALT-arrumaram', 'sobiando-OALT-assobiando', 'tá-OALT-está', 'tamo-OALT-estamos', 'tamos-OALT-estamos', 'tão-OALT-estão',
                   'tar-OALT-estar', 'taria-OALT-estaria', 'tás-OALT-estás', 'tava-OALT-estava', 'tavam-OALT-estavam', 'távamos-OALT-estávamos',
                   'tavas-OALT-estavas', 'teja-OALT-esteja', 'tendeu-OALT-entendeu', 'tendi-OALT-entendi', 'teve-OALT-esteve',
                   'tive-OALT-estive', 'tiver-OALT-estiver', 'tiverem-OALT-estiverem', 'tivesse-OALT-estivesse', 'tô-OALT-estou', 'trapalha-OALT-atrapalha',
                   'trapalhou-OALT-atrapalhou', 'travessa-OALT-atravessa', 'xá-OALT-deixa']
    apheresisAdjs = ['baulado-OALT-abaulado', 'borrecido-OALT-aborrecido', 'brigado-OALT-obrigado', 'brigada-OALT-obrigada',
                 'dotada-OALT-adotada', 'garrado-OALT-agarrados', 'marelo-OALT-amarelo', 'pinhada-OALT-apinhada', 'rorosa-OALT-horrorosa',
                  'teirinho-OALT-inteirinho', 'trapalhado-OALT-atrapalhado', 'trevidão-OALT-atrevidão'] # 'travessadinho' is in in the book list but not in the corpus
    apheresisAdvs = ['bora-OALT-embora', 'final-OALT-afinal','gora-OALT-agora', 'inda-OALT-ainda', 'manhã-OALT-amanhã', 'pois-OALT-depois', 'qui-OALT-aqui',
                 'té-OALT-até']
# 'tão' (as a ADV) is in in the book list but not in the corpus
# 'pesar' is in in the book list but not in the corpus
#PROCURAR NO CORPUS COMO QUE ESTÁ ANOTADO
    apheresisNouns = ['fessora-OALT-professora', 'lambique-OALT-alambique', 'migão-OALT-amigão', 'mor-OALT-amor', 'ném-OALT-neném',
                  'partamento-OALT-apartamento', 'pelido-OALT-apelido', 'posa-OALT-raposa', 'regaço-OALT-arregaços',
                  'roz-OALT-arroz', 'testino-OALT-intestino', 'vó-OALT-avó', 'vô-OALT-avô']
# 'tradinha' is in in the book list but not in the corpus
    apheresisProperNouns = ['agateelevê-OALT-HTLV']
    apheresisConjuctionsKC = ['gual-OALT-igual', 'gualzim-OALT-igualzinho'] # a tag vai ser KC
    apheresisConjuctionsKS = ['gual-OALT-igual', 'gualzim-OALT-igualzinho'] # a tag vai ser KS
    apheresisInterjections = ['tadim-OALT-tadinho', 'tadinha-OALT-coitadinha', 'tadinho-OALT-coitadinho', 'tadinhos-OALT-coitadinhos']
    apheresisPrepositions= ['té-OALT-até']

# for each element of each apheresis list, the correspondent pos tag is added
    aphereticVerbs = [form + "_<V" for form in apheresisVerbs]
    aphereticAdjs = [form + "_<ADJ" for form in apheresisAdjs]
    aphereticAdvs = [form + "_<ADV" for form in apheresisAdvs]
    aphereticNouns = [form + "_<N" for form in apheresisNouns]
    aphereticProperNouns = [form + "_<PROPN" for form in apheresisProperNouns]
    aphereticConjunctionsKS = [form + "_<KS" for form in apheresisConjuctionsKS]
    aphereticConjunctionsKC = [form + "_<KC" for form in apheresisConjuctionsKC]
    aphereticInterjections = [form + "_<IN" for form in apheresisInterjections]
    aphereticPrepositions = [form + "_<PRP" for form in apheresisPrepositions]

# combining all lists into one
    allApheresis = [aphereticVerbs,aphereticAdjs, aphereticAdvs, aphereticNouns, aphereticProperNouns, aphereticConjunctionsKS, aphereticConjunctionsKC,
                    aphereticInterjections,aphereticPrepositions]

    tokens = string.split(" ") # divide the string by white space (=~token)
    apheresis = [] # an empty list to store the apheretic forms found
    for i in range(len(tokens)): # for each position that within the tokens list size
        for listing in allApheresis: # for each list in allApheresis
            for item in listing: # for each item of each list in allApheresis
                if item == tokens[i]: # if item is the same as the token in this position
                    apheresis.append(item) # put the item into the empty list
    return apheresis

posTagUtteranceWithInfoDf["list_apheresis"] = posTagUtteranceWithInfoDf.words_with_tags.apply(lambda x: listingApheresis(x)) #applying the function

# column with the size of the apheresis list
posTagUtteranceWithInfoDf["count_apheresis"] = posTagUtteranceWithInfoDf.list_apheresis.apply(lambda x: len(x))

apheretic_speaker = posTagUtteranceWithInfoDf['count_apheresis'].sum()/len(posTagUtteranceWithInfoDf)
apheretic_speaker = round(apheretic_speaker, 2)
apheretic_utterance = posTagUtteranceWithInfoDf['count_apheresis'].sum()/posTagUtteranceWithInfoDf['total_utterance'].sum()
apheretic_utterance = round(apheretic_utterance, 2)
print(f"the total number of apheretic forms in the corpus is {posTagUtteranceWithInfoDf['count_apheresis'].sum()}\nTherefore, it would be {apheretic_speaker} apheretic form per speaker.\nAnd {apheretic_utterance} per utterance")

"""### "mó" (=maior) intensifier"""

def countMoIntensifier(string):
    '''
    it checks if the mó intensifier is in the string
    and it counts how it appears
    '''
    mo_intensifier = []
    if "mó-OALT-maior" in string:
        mo_intensifier.append("mó")
        return len(mo_intensifier)
    else:
        return 0

# applying the function
posTagUtteranceWithInfoDf["count_mo_intensifier"] = posTagUtteranceWithInfoDf["clean_utterance"].apply(lambda x: countMoIntensifier(x))

"""### rhotacism"""

def listingRhotacism(string):
    '''
    it returns a list with the words found with rotacionism
    '''
    # list with rotacionism forms
    rotacionism = ['armoçar-OALT-almoçar', 'artinho-OALT-altinho', 'arto-OALT-alto', 'comprica-OALT-complica', 'cravícula-OALT-clavícula',
               'escardada-OALT-escaldada', 'prano-OALT-plano', 'pranta-OALT-planta', 'pray-OALT-play', 'prissado-OALT-plissado', 'probremas-OALT-problemas',
                'sortando-OALT-soltando','sortar-OALT-soltar', 'sortei-OALT-soltei', 'sorto-OALT-solto', 'vorta-OALT-volta', 'vortar-OALT-voltar',
                'vortava-OALT-voltava', 'vorto-OALTvolto']
    # this function lists all of the rotacionized forms in the string
    tokens = string.split(" ") # divide the string by white space (=~token)
    rotacionizedtForm = [] # an empty list to store the rotacionized forms found
    for i in range(len(tokens)): # for each position that within the tokens list size
        for item in rotacionism: # for each item of each list in rotacionismOALT
             if item == tokens[i]: # if item is the same as the token in this position
                 rotacionizedtForm.append(item) # put the item into the empty list
    return len(rotacionizedtForm)

posTagUtteranceWithInfoDf["count_rhotacism"] = posTagUtteranceWithInfoDf.clean_utterance.apply(lambda x: listingRhotacism(x)) #applying the function

"""### Senhor/senhora pronunciation"""

def listingSenhorSenhora(dataframe, stringColumn, newColumn):
    '''
    it counts the number of different pronunciations of the words senhor and senhora
    '''
    # list with the pronunciation forms
    senhorSenhora = ["seu-OALT-senhor", "sio'-OALT-senhora", 'sior-OALT-senhor', 'siora-OALT-senhora', 'sô-OALT-senhor',
                 r"seu-OALT-senhor=[a-zA-ZÀ-ÿũ]+"]
    for form in senhorSenhora:
        #find each form from the list in each row of the newString column (it's stored in a list)
        dataframe[f"{newColumn}"] = dataframe[f"{stringColumn}"].apply(lambda x: re.findall(form, x))
    return dataframe

posTagUtteranceWithInfoDf = listingSenhorSenhora(posTagUtteranceWithInfoDf, "clean_utterance", "list_senhor_senhora")
posTagUtteranceWithInfoDf["count_senhor_senhora"] = posTagUtteranceWithInfoDf.list_senhor_senhora.apply(lambda x: len(x))

"""### diminutive"""

def countDimunitive(string):
    '''
    it counts the apocopated diminutives
    '''
    # list with the pronunciation forms
    apocopeDiminutives = ['almoçozim', 'amarelim', 'azulzim', 'bebezim', 'bichim', 'bocadim', 'bonitim', 'cachorrim', 'cantim', 'capoeirim', 'cantim', 'carrim',
               'cedezinho', 'certim', 'certins', 'Chapeuzim Vermelho', 'chazim', 'controladim', 'desfiadim', 'direitim', 'esquisitim', 'fechadim',
               'filhotim', 'formulaziozim', 'fundim', 'Geraldim', 'golezim', 'igualzim', 'instatim', 'jeitim', 'Joãozim', 'joguim', 'ladim',
               'maciim', 'mansim', 'Marquim','meninim', 'morenim', 'murim', 'Paulim', 'pequeninim', 'pertim', 'negocim', 'partidim', 'porquim',
               'portim', 'potim', 'pouquim', 'pozim', 'pretim', 'prontim', 'quadradim', 'queimadim', 'rapidim', 'recheadim', 'rolim', 'tamanim',
               'tampadim', 'terrenim', 'tiquim', 'todim', 'toquim', 'trancadim', 'trenzim', 'tudim']

    foundApocopeDim = [] # an empty list with the found apocopated diminutives
    return len(foundApocopeDim) #return the number of apocopated diminutives

    # in case you just want to know the specific diminutives, you can put the "return" command directly with the lists without the len()

posTagUtteranceWithInfoDf["count_diminutive"] = posTagUtteranceWithInfoDf.clean_utterance.apply(lambda x: countDimunitive(x)) #applying the function

"""### foreign words"""

def countForeign(string):
    '''
    it counts the number of foreign words and expressions
    '''
    # list with the words
    foreign = ['add', 'aftermarket', 'air bags', 'al dente', r'\banche\b', 'a priori', 'art nouveau', 'atelier', r'\ba ver', 'babies', 'because', r'big\b',
           'blue jeans', 'body art', 'brise', 'buffet', 'bye-bye', 'cappelletti', 'careful', r'chat\b', 'cheddar', 'come stai', 'come tu ti chiami',
           'come with me', 'comieira', 'comment', 'completed', 'corpus', 'crack', 'crossover', r'\bcult\b', 'delicious', 'designers', 'desktop', 'diavolo',
           r'diet\b', 'don\'t use', r'drag\b', 'queens', r'edit\b', 'eight', 'e-mail', r'\benter\b', r'est\b', 'feedback', 'feeling', 'file', r'flash\b',
           'flashback', 'fondue', r'format\b', 'freelance', 'freezer', 'friends', 'funk', 'funk', 'gay', r'gloss\b', 'go for it', 'go-go boy', 'handsome',
           'happening', 'hard-core', 'hello', 'hobby', 'homework', r'hot\b', 'hype', r'I don\'t give my heart to', 'imagination', 'internet', 'io mi chiamo',
           'karowara', 'ketchup', r'kit\b', 'kitsch', 'laptop', 'lato sensu', 'Lei', 'lettering', 'level', 'light', 'line out', 'link', 'looping',
           'love', 'majors', 'mano yo', 'marchand', 'margherita', 'markup', 'media output', 'menu', 'merchand', 'mise-en-scène', 'modus operandi',
           'monologue', 'mouse', 'much better', r'music\b', 'my computer', 'navy', 'nerd', 'network', 'non aedificandi', r'not\b', 'notebook', 'off',
           'oops', 'open', 'output video', 'paella', 'pause', 'pay-per-view', 'pen drive', 'perharps love is', 'permesso di soggiorno', 'personal killer',
           'personal trainer', r'pizza\b', 'pizzaiola', 'play', 'playboy', r'pop\b', 'preset', r'pure\b', 'quórum', 'ready', r'rec\b', 'receiver',
           'remote', 'réveillon', 'review', 'rock-\'n\'-roll', 'save', 'script', r'\bset\b', 'sexy', 'shift', 'shit', 'shopping', 'short', 'show', 'shoyu',
           'site', r'\bspa\b', 'squash', 'start', 'status', 'sto bene', 'sto male', 'stop', 'story', 'stricto sensu', 'telemarketing', 'tesoro',
           'top hit', 'town house', 'track erase', 'trade', 'trailer', 'trash', 'tsuru', 'two', 'update', 'upload', 'video output', 'VIP', r'\bvoi\b',
           'vous', 'wafer', 'watch and learn', 'web', 'well', 'white balance', 'yes', r'\bzum\b']

    matches = []
    for form in foreign:
        # for each form in the list, check if it's in the string.
        # append the new list into the matches list
        match = re.findall(form, string)
        matches.append(match)

    # the matches list is a list of lists. The line below converts it to a one single list
    matchesFinal = [item for sublist in matches for item in sublist]
    return len(matchesFinal)
    # in case you just want to know the specific diminutives, you can put the "return" command directly with the lists without the len()

posTagUtteranceWithInfoDf["count_foreign_words"] = posTagUtteranceWithInfoDf.clean_utterance.apply(lambda x: countForeign(x))

"""### reduced and articulated prepositions"""

def countPrepositions(string):
    '''
    it counts the number of articulated and reduced prepositions
    '''
    # list with the words
    prepositions = ['c\'', r'\bca\b', r'\bco\b', r'\bcos\b', r'\bcum\b', r'\bcuma\b', 'd\'', r'\bdum\b', r'\bduma\b', r'\bdumas\b', r'\bduns\b',
               'n\' aonde', 'n\' deereí', r'\bni\b', 'n\' onde', r'\bnum\b', r'\bnuma\b', r'\bnumas\b', r'\bpa\b', r'\bpas\b', 'p\'',
               r'\bpo\b', r'\bpro\b', r'\bpra\b', r'\bpr\'\b', r'\bpras\b', r'\bpro\b', r'\bpro\b', r'\bprum\b', r'\bpruma\b', r'\bpruns\b',
               r'\bpum\b', r'\bpuma\b', r'n\' \bocê\b', r'n\' \bocês\b']

    matches = []
    for form in prepositions:
        # for each form in the list, check if it's in the string.
        # append the new list into the matches list
        match = re.findall(form, string)
        matches.append(match)


    # the matches list is a list of lists. The line below converts it to a one single list
    matchesFinal = [item for sublist in matches for item in sublist]
    return len(matchesFinal)
    # in case you just want to know the specific diminutives, you can put the "return" command directly with the lists without the len()

posTagUtteranceWithInfoDf["count_prepositions"] = posTagUtteranceWithInfoDf.clean_utterance.apply(lambda x: countPrepositions(x))

"""### pronominal phenomena"""

def countPronouns(string):
    pronouns = [r'\bcê\b', r'\bcês\b', r'\be\'', r'\bea\b', r'\beas\b', r'\bes\b', r'\bocê\b', r'\bocês\b', r'\baque\'', r'\baquea\b',
            r'\baqueas\b', r'\baques\b']
    PronounFound = []
    for pron in pronouns:
        if re.findall(pron, string, re.UNICODE):
            PronounFound.append(pron)
    return len(PronounFound)

posTagUtteranceWithInfoDf["count_pronouns"] = posTagUtteranceWithInfoDf.clean_utterance.apply(lambda x: countPronouns(x))

"""### interjections and exclamations"""

def countInterjections(string):
    '''
    it counts the number of different interjections by the pos tag
    '''
    matches = re.findall(r"(\w+)_\<IN", string, re.UNICODE)

    return len(matches)
# in case you just want to know the specific diminutives, you can put the "return" command directly with the lists without the len()

posTagUtteranceWithInfoDf["count_interjections"] = posTagUtteranceWithInfoDf.words_with_tags.apply(lambda x: countInterjections(x))

posTagUtteranceWithInfoDf.columns #checking new columns

# selecting and reordering the columns
posTagUtteranceWithInfoDf_selected = posTagUtteranceWithInfoDf[['file', 'acronym', 'person_code', 'utterance', 'clean_utterance',
       'words_with_tags', 'lemmas', 'pos_tagged', 'total_words',
       'total_utterance', 'total_nonstandard_verb_agreement',
       'total_nonstandard_verb_conjugation', 'total_count_negation', 'total_non_standard_plural', 'count_apheresis','count_mo_intensifier',
       'count_rhotacism','count_senhor_senhora',
       'count_diminutive', 'count_foreign_words', 'count_prepositions',
       'count_pronouns', 'count_interjections']]

"""### Exporting"""

# exporting
date = datetime.now().strftime('%Y-%m-%d')
posTagUtteranceWithInfoDf_selected.to_csv(textOutputPath+f"posTagUtteranceWithInfo_csv_{date}.csv")